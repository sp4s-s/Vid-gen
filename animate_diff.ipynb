{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8UseYDOiVDd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/guoyww/AnimateDiff.git\n",
        "%cd AnimateDiff\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -O https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_sparsectrl_rgb.ckpt"
      ],
      "metadata": {
        "id": "qf6trFxQqUnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from diffusers import DDIMScheduler, AutoencoderKL\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "a1uJp8nrqddD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_video\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "class AnimateDiffDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        image_size: Tuple[int, int] = (512, 512),\n",
        "        num_frames: int = 16,\n",
        "        frame_stride: int = 4,\n",
        "        image_transform: transforms.Compose = None,\n",
        "        video_transform: transforms.Compose = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Path to root directory containing sample folders\n",
        "            image_size: Target resolution for resizing\n",
        "            num_frames: Number of frames to extract from each video\n",
        "            frame_stride: Number of frames to skip between sampled frames\n",
        "            transforms: Optional custom transforms\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.image_size = image_size\n",
        "        self.num_frames = num_frames\n",
        "        self.frame_stride = frame_stride\n",
        "        self.samples = self._discover_samples()\n",
        "\n",
        "        # Default transforms if not provided\n",
        "        self.image_transform = image_transform or transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "        self.video_transform = video_transform or transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "    def _discover_samples(self) -> list:\n",
        "        \"\"\"Find valid sample folders containing both image and video\"\"\"\n",
        "        samples = []\n",
        "        for folder_name in os.listdir(self.root_dir):\n",
        "            folder_path = os.path.join(self.root_dir, folder_name)\n",
        "            if os.path.isdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, \"image.jpg\")\n",
        "                video_path = os.path.join(folder_path, \"video.mp4\")\n",
        "                if os.path.isfile(image_path) and os.path.isfile(video_path):\n",
        "                    samples.append((folder_name, image_path, video_path))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        folder_name, image_path, video_path = self.samples[idx]\n",
        "\n",
        "        # Load and process image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        # Load and process video\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')  # (T, H, W, C)\n",
        "\n",
        "        # Temporal sampling\n",
        "        total_frames = video.shape[0]\n",
        "        frame_indices = torch.linspace(0, total_frames-1,\n",
        "                                     steps=self.num_frames).long()\n",
        "        video = video[frame_indices]\n",
        "\n",
        "        # Convert and transform frames\n",
        "        frames = []\n",
        "        for frame in video:\n",
        "            frame = Image.fromarray(frame.numpy())\n",
        "            frames.append(self.video_transform(frame))\n",
        "        video = torch.stack(frames)  # (T, C, H, W)\n",
        "\n",
        "        return {\n",
        "            \"folder_name\": folder_name,\n",
        "            \"conditioning_image\": image,  # (C, H, W)\n",
        "            \"target_frames\": video,       # (T, C, H, W)\n",
        "            \"image_path\": image_path,\n",
        "            \"video_path\": video_path\n",
        "        }\n",
        "\n",
        "    def get_sample_metadata(self, idx: int) -> Dict:\n",
        "        \"\"\"Get metadata without loading actual media\"\"\"\n",
        "        folder_name, image_path, video_path = self.samples[idx]\n",
        "        return {\n",
        "            \"folder_name\": folder_name,\n",
        "            \"image_path\": image_path,\n",
        "            \"video_path\": video_path\n",
        "        }"
      ],
      "metadata": {
        "id": "2Je1HRityGXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Union, List\n",
        "\n",
        "def create_animediff_dataset(\n",
        "    video_dirs: Union[str, List[str]],\n",
        "    output_root: str,\n",
        "    target_fps: int = 16,\n",
        "    duration_seconds: int = 4,\n",
        "    image_size: Tuple[int, int] = (512, 512),\n",
        "    max_samples: int = None\n",
        ") -> AnimateDiffDataset:\n",
        "    \"\"\"\n",
        "    Process video directories to create AnimateDiff-compatible dataset\n",
        "    Returns initialized dataset ready for training\n",
        "\n",
        "    Args:\n",
        "        video_dirs: Directories containing input videos\n",
        "        output_root: Where to save processed samples\n",
        "        target_fps: Frames per second for output videos\n",
        "        duration_seconds: Length of video clips to extract (4 seconds)\n",
        "        image_size: Resolution for resizing\n",
        "        max_samples: Maximum number of samples to process (None for all)\n",
        "    \"\"\"\n",
        "    # Create output structure\n",
        "    output_root = Path(output_root)\n",
        "    output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Collect video files\n",
        "    video_paths = []\n",
        "    if isinstance(video_dirs, str):\n",
        "        video_dirs = [video_dirs]\n",
        "\n",
        "    for dir_path in video_dirs:\n",
        "        for root, _, files in os.walk(dir_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "                    video_paths.append(Path(root) / file)\n",
        "\n",
        "    # Process videos\n",
        "    samples_created = 0\n",
        "    for video_path in tqdm(video_paths, desc=\"Processing videos\"):\n",
        "        if max_samples and samples_created >= max_samples:\n",
        "            break\n",
        "\n",
        "        # Create sample directory\n",
        "        sample_id = f\"sample_{samples_created:06d}\"\n",
        "        sample_dir = output_root / sample_id\n",
        "        sample_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Process video\n",
        "        success = process_single_video(\n",
        "            video_path=video_path,\n",
        "            output_dir=sample_dir,\n",
        "            target_fps=target_fps,\n",
        "            duration_seconds=duration_seconds,\n",
        "            image_size=image_size\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            samples_created += 1\n",
        "\n",
        "    print(f\"Created {samples_created} valid samples\")\n",
        "    return AnimateDiffDataset(output_root, image_size=image_size)\n",
        "\n",
        "def process_single_video(\n",
        "    video_path: Path,\n",
        "    output_dir: Path,\n",
        "    target_fps: int,\n",
        "    duration_seconds: int,\n",
        "    image_size: Tuple[int, int]\n",
        ") -> bool:\n",
        "    \"\"\"Process individual video into AnimateDiff format\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "    # Get video properties\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate required frames\n",
        "    required_frames = target_fps * duration_seconds\n",
        "    frame_step = max(1, int(original_fps / target_fps))\n",
        "\n",
        "    # Skip videos that are too short\n",
        "    if total_frames < required_frames * frame_step:\n",
        "        return False\n",
        "\n",
        "    # Read and process frames\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    success = True\n",
        "\n",
        "    while success and len(frames) < required_frames:\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_step == 0:\n",
        "            # Convert BGR to RGB and resize\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, image_size)\n",
        "            frames.append(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Save first frame as image\n",
        "    if len(frames) < required_frames:\n",
        "        return False\n",
        "\n",
        "    image = Image.fromarray(frames[0])\n",
        "    image.save(output_dir / \"image.jpg\")\n",
        "\n",
        "    # Save video clip\n",
        "    video_array = np.array(frames[:required_frames])\n",
        "    save_video_as_frames(video_array, output_dir / \"video.mp4\", target_fps)\n",
        "\n",
        "    return True\n",
        "\n",
        "def save_video_as_frames(frames: np.ndarray, output_path: Path, fps: int):\n",
        "    \"\"\"Save numpy array as video file\"\"\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    h, w = frames[0].shape[:2]\n",
        "    writer = cv2.VideoWriter(\n",
        "        str(output_path),\n",
        "        fourcc,\n",
        "        fps,\n",
        "        (w, h)\n",
        "\n",
        "    for frame in frames:\n",
        "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "    writer.release()"
      ],
      "metadata": {
        "id": "BPSyxf1wqdWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process raw videos and create dataset\n",
        "dataset = create_animediff_dataset(\n",
        "    video_dirs=[\"path/to/video_folder1\", \"path/to/video_folder2\"],\n",
        "    output_root=\"processed_dataset\",\n",
        "    target_fps=16,\n",
        "    duration_seconds=4,\n",
        "    image_size=(512, 512),\n",
        "    max_samples=1000  # Optional: limit number of samples\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Training loop example\n",
        "for batch in dataloader:\n",
        "    conditioning_images = batch[\"conditioning_image\"].to(device)  # (B, C, H, W)\n",
        "    target_frames = batch[\"target_frames\"].to(device)            # (B, T, C, H, W)\n",
        "\n",
        "    # Forward pass through model\n",
        "    outputs = model(conditioning_images, target_frames)\n",
        "\n",
        "    # Compute loss and backpropagate\n",
        "    loss = criterion(outputs, target_frames)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "X36ERBylu-1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify dataset structure\n",
        "def verify_dataset(dataset: AnimateDiffDataset):\n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        metadata = dataset.get_sample_metadata(i)\n",
        "\n",
        "        assert sample[\"conditioning_image\"].shape == (3, 512, 512)\n",
        "        assert sample[\"target_frames\"].shape == (16, 3, 512, 512)\n",
        "        assert Path(metadata[\"image_path\"]).exists()\n",
        "        assert Path(metadata[\"video_path\"]).exists()\n",
        "\n",
        "    print(\"Dataset verification passed!\")\n",
        "\n",
        "verify_dataset(dataset)"
      ],
      "metadata": {
        "id": "pyu2S6Itu-3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRR4O9vdu-43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJLKZMOFqdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_path = \"path/to/pretrained/animtediff/weights\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_frames = 16 * 4\n",
        "height, width = 512, 512\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\")\n",
        "vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder=\"vae\")\n",
        "unet = UNet3DConditionModel.from_pretrained(pretrained_model_path, subfolder=\"unet\")\n",
        "scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder=\"scheduler\")\n",
        "\n",
        "pipeline = AnimationPipeline(\n",
        "    vae=vae,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    unet=unet,\n",
        "    scheduler=scheduler,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "SUuzUCyKqUpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoGenerationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.video_files = [f for f in os.listdir(root_dir) if f.endswith('.mp4')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.root_dir, self.video_files[idx])\n",
        "        frames = load_and_process_video(video_path)  # Shape of [T, H, W, C]\n",
        "\n",
        "        # Extract first frame as conditioning image\n",
        "        conditioning_image = frames[0]\n",
        "        target_frames = frames\n",
        "\n",
        "        if self.transform:\n",
        "            conditioning_image = self.transform(conditioning_image)\n",
        "            target_frames = torch.stack([self.transform(f) for f in target_frames])\n",
        "\n",
        "        return {\n",
        "            \"conditioning_image\": conditioning_image,\n",
        "            \"target_frames\": target_frames,\n",
        "            \"prompt\": \"\"  # Add text prompts if available\n",
        "        }\n",
        "\n",
        "def load_and_process_video(path):\n",
        "    # Implement video loading logic (e.g., using decord or torchvision)\n",
        "    # Return tensor of shape [num_frames, height, width, channels]\n",
        "    pass"
      ],
      "metadata": {
        "id": "C5Iq7pUhqUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, epochs=10, batch_size=2):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(model.unet.parameters(), lr=1e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataloader:\n",
        "            conditioning_images = batch[\"conditioning_image\"].to(device)\n",
        "            target_frames = batch[\"target_frames\"].to(device)\n",
        "\n",
        "            latents = vae.encode(target_frames).latent_dist.sample()\n",
        "            latents = latents * 0.18215  # Scaling factor\n",
        "\n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(latents)\n",
        "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "            # Add noise to latents\n",
        "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Forward pass\n",
        "            model_output = model.unet(\n",
        "                noisy_latents,\n",
        "                timesteps,\n",
        "                encoder_hidden_states=text_encoder(batch[\"prompt\"])[0],\n",
        "                conditioning_images=conditioning_images\n",
        "            ).sample\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = torch.nn.functional.mse_loss(model_output, noise)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "D9xppqO0qUsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video_from_image(pipeline, image, prompt=\"\", num_frames=64):\n",
        "    # Preprocess input image\n",
        "    image = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
        "    image = transforms.Resize((height, width))(image)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        frames = pipeline(\n",
        "            prompt=prompt,\n",
        "            conditioning_image=image,\n",
        "            num_frames=num_frames,\n",
        "            guidance_scale=7.5,\n",
        "            num_inference_steps=50\n",
        "        ).video\n",
        "\n",
        "    # Post-process output\n",
        "    frames = rearrange(frames[0], \"c t h w -> t h w c\").cpu().numpy()\n",
        "    return (frames * 255).astype(np.uint8)"
      ],
      "metadata": {
        "id": "DTzxxJ8QqUuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire pipeline\n",
        "pipeline.save_pretrained(\"path/to/save/model\")\n",
        "\n",
        "# Load saved model\n",
        "pipeline = AnimationPipeline.from_pretrained(\"path/to/saved/model\").to(device)"
      ],
      "metadata": {
        "id": "2nt6kOwpqUwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = AnimationPipeline(...).to(device)\n",
        "\n",
        "# Load or fine-tune\n",
        "# train(pipeline, dataset)  # Uncomment for fine-tuning\n",
        "\n",
        "# Generate video from image\n",
        "from PIL import Image\n",
        "\n",
        "input_image = Image.open(\"input.jpg\")\n",
        "generated_video = generate_video_from_image(\n",
        "    pipeline=pipeline,\n",
        "    image=input_image,\n",
        "    prompt=\"high quality, cinematic, 4K resolution\",\n",
        "    num_frames=64\n",
        ")\n",
        "\n",
        "# Save video\n",
        "import imageio\n",
        "imageio.mimwrite(\"output.mp4\", generated_video, fps=16)"
      ],
      "metadata": {
        "id": "9oofzVDuqUy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTQveShXqU1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AhcJxdG0qWOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TtQxR7CtqWP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GJ55TvEdqWRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3y3LxLzjqWTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUbmj3PyqW7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E93LMpwIqW8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qi94IXMCqW-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbhYa5p1qXGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQ43VGiDqXIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jt6jooYXqXJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKgve3myqXLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPl6ETHTqXN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}